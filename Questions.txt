Le tokenizer doit etre langage dependant ? 
Les embeddings sortent > 100 t-sne ? truncate ? 
